{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3039fae7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries và Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a86a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import struct\n",
    "\n",
    "# Import model\n",
    "sys.path.append('./model')\n",
    "from model.SoftmaxRegression import SoftmaxRegression\n",
    "\n",
    "# Hàm đọc file MNIST IDX format\n",
    "def load_mnist_images(filename):\n",
    "    \"\"\"Đọc file MNIST images (idx3-ubyte format)\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Đọc header\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        # Đọc pixel data\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows * cols)\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    \"\"\"Đọc file MNIST labels (idx1-ubyte format)\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Đọc header\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        # Đọc label data\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cf9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data từ file IDX format\n",
    "x_train_flat = load_mnist_images('./data/train-images.idx3-ubyte')\n",
    "y_train = load_mnist_labels('./data/train-labels.idx1-ubyte')\n",
    "\n",
    "x_test_flat = load_mnist_images('./data/t10k-images.idx3-ubyte')\n",
    "y_test = load_mnist_labels('./data/t10k-labels.idx1-ubyte')\n",
    "\n",
    "# Normalize pixel values về [0, 1]\n",
    "x_train_flat = x_train_flat / 255.0\n",
    "x_test_flat = x_test_flat / 255.0\n",
    "\n",
    "print(f\"Training set: {x_train_flat.shape}\")\n",
    "print(f\"Test set: {x_test_flat.shape}\")\n",
    "print(f\"Labels: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba9192",
   "metadata": {},
   "source": [
    "## 2. Định nghĩa Feature Extraction Functions\n",
    "\n",
    "Sử dụng các hàm tương tự như trong `feature_design.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c341f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_column(X):\n",
    "    \"\"\"Thêm cột bias (toàn 1) vào feature matrix\"\"\"\n",
    "    return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "def get_design1_raw_pixels(X, mask=None):\n",
    "    \"\"\"\n",
    "    Design 1: Raw pixels with variance filtering\n",
    "    Loại bỏ các pixel có variance = 0\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        # Tính variance của từng pixel\n",
    "        variances = np.var(X, axis=0)\n",
    "        mask = variances > 0  # Giữ lại pixel có variance > 0\n",
    "    \n",
    "    # Lọc pixels\n",
    "    X_filtered = X[:, mask]\n",
    "    \n",
    "    # Thêm bias\n",
    "    X_with_bias = add_bias_column(X_filtered)\n",
    "    \n",
    "    return X_with_bias, mask\n",
    "\n",
    "def get_design2_block_avg(X, block_size=2):\n",
    "    \"\"\"\n",
    "    Design 2 & 3: Block averaging\n",
    "    Chia ảnh 28x28 thành các block và tính trung bình\n",
    "    \n",
    "    Args:\n",
    "        block_size: 2 cho Design 2, 4 cho Design 3\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    img_size = 28\n",
    "    \n",
    "    # Reshape về dạng ảnh\n",
    "    X_reshaped = X.reshape(n_samples, img_size, img_size)\n",
    "    \n",
    "    # Số block mỗi chiều\n",
    "    n_blocks = img_size // block_size\n",
    "    \n",
    "    # Khởi tạo output\n",
    "    features = []\n",
    "    \n",
    "    for img in X_reshaped:\n",
    "        block_features = []\n",
    "        for i in range(n_blocks):\n",
    "            for j in range(n_blocks):\n",
    "                # Lấy block\n",
    "                block = img[\n",
    "                    i*block_size:(i+1)*block_size,\n",
    "                    j*block_size:(j+1)*block_size\n",
    "                ]\n",
    "                # Tính trung bình\n",
    "                block_features.append(block.mean())\n",
    "        features.append(block_features)\n",
    "    \n",
    "    X_blocks = np.array(features)\n",
    "    \n",
    "    # Thêm bias\n",
    "    X_with_bias = add_bias_column(X_blocks)\n",
    "    \n",
    "    return X_with_bias\n",
    "\n",
    "def get_design3_projection(x_input, target_dim=100):\n",
    "    N, D = x_input.shape\n",
    "    \n",
    "    np.random.seed(42) \n",
    "    \n",
    "    projection_matrix = np.random.randn(D, target_dim)\n",
    "  \n",
    "    x_projected = np.dot(x_input, projection_matrix)\n",
    "    \n",
    "    max_val = np.max(np.abs(x_projected))\n",
    "    x_norm = x_projected / max_val\n",
    "\n",
    "    return np.hstack([np.ones((N, 1)), x_norm])\n",
    "\n",
    "\n",
    "def get_design5_pca(X_train, X_test, n_components=331):\n",
    "    \"\"\"\n",
    "    Design 5: PCA dimensionality reduction\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Fit PCA trên training set\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Thêm bias\n",
    "    X_train_with_bias = add_bias_column(X_train_pca)\n",
    "    X_test_with_bias = add_bias_column(X_test_pca)\n",
    "    \n",
    "    print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    \n",
    "    return X_train_with_bias, X_test_with_bias, pca\n",
    "\n",
    "print(\"Feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81884c2c",
   "metadata": {},
   "source": [
    "## 3. Tạo Feature Vectors cho tất cả Designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating feature vectors...\\n\")\n",
    "\n",
    "# Design 1: Raw Pixels (Filtered)\n",
    "print(\"Design 1: Raw Pixels (Filtered)\")\n",
    "x_train_d1, mask_d1 = get_design1_raw_pixels(x_train_flat)\n",
    "x_test_d1, _ = get_design1_raw_pixels(x_test_flat, mask=mask_d1)\n",
    "print(f\"  Shape: {x_train_d1.shape}\\n\")\n",
    "\n",
    "# Design 2: Block Avg 2x2\n",
    "print(\"Design 2: Block Averaging 2x2\")\n",
    "x_train_d2 = get_design2_block_avg(x_train_flat, block_size=2)\n",
    "x_test_d2 = get_design2_block_avg(x_test_flat, block_size=2)\n",
    "print(f\"  Shape: {x_train_d2.shape}\\n\")\n",
    "\n",
    "# Design 3: Block Avg 4x4\n",
    "print(\"Design 3: Block Averaging 4x4\")\n",
    "x_train_d3 = get_design2_block_avg(x_train_flat, block_size=4)\n",
    "x_test_d3 = get_design2_block_avg(x_test_flat, block_size=4)\n",
    "print(f\"  Shape: {x_train_d3.shape}\\n\")\n",
    "\n",
    "# Design 4: Projection Profiles\n",
    "print(\"Design 4: Projection Profiles\")\n",
    "x_train_d4 = get_design3_projection(x_train_flat)\n",
    "x_test_d4 = get_design3_projection(x_test_flat)\n",
    "print(f\"  Shape: {x_train_d4.shape}\\n\")\n",
    "\n",
    "# Design 5: PCA\n",
    "print(\"Design 5: PCA (331 components)\")\n",
    "x_train_d5, x_test_d5, pca_model = get_design5_pca(x_train_flat, x_test_flat, n_components=331)\n",
    "print(f\"  Shape: {x_train_d5.shape}\\n\")\n",
    "\n",
    "print(\"All feature vectors created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c185c",
   "metadata": {},
   "source": [
    "## 4. Train Models cho từng Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách các designs\n",
    "designs = [\n",
    "    (\"Design 1: Raw Pixels (Filtered)\", x_train_d1, x_test_d1),\n",
    "    (\"Design 2: Block Avg 2x2\", x_train_d2, x_test_d2),\n",
    "    (\"Design 3: Block Avg 4x4\", x_train_d3, x_test_d3),\n",
    "    (\"Design 4: Projection Profiles\", x_train_d4, x_test_d4),\n",
    "    (\"Design 5: PCA\", x_train_d5, x_test_d5)\n",
    "]\n",
    "\n",
    "# Dictionary để lưu kết quả\n",
    "results = {}\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "REG = 1e-4\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f357303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train từng design\n",
    "for name, X_train, X_test in designs:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Khởi tạo model\n",
    "    model = SoftmaxRegression(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        reg=REG,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'n_features': X_train.shape[1],\n",
    "        'loss_history': model.history['loss']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Overfitting Gap: {train_acc - test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41e34c",
   "metadata": {},
   "source": [
    "## 5. So sánh Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo comparison table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Design':<40} {'Features':<12} {'Train Acc':<12} {'Test Acc':<12} {'Gap':<10}\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "for name, result in results.items():\n",
    "    gap = result['train_accuracy'] - result['test_accuracy']\n",
    "    print(f\"{name:<40} {result['n_features']:<12} {result['train_accuracy']:<12.4f} {result['test_accuracy']:<12.4f} {gap:<10.4f}\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Tìm best model\n",
    "best_design = max(results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "print(f\"\\nBest Design: {best_design[0]}\")\n",
    "print(f\"   Test Accuracy: {best_design[1]['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c37b28",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['loss_history'], label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Curves for All Designs', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved to 'training_curves.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7fd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart cho accuracy\n",
    "design_names = [name.split(':')[0] for name in results.keys()]\n",
    "train_accs = [result['train_accuracy'] for result in results.values()]\n",
    "test_accs = [result['test_accuracy'] for result in results.values()]\n",
    "\n",
    "x = np.arange(len(design_names))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, train_accs, width, label='Train Accuracy', alpha=0.8)\n",
    "ax1.bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n",
    "ax1.set_xlabel('Design', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Train vs Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(design_names, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Feature count comparison\n",
    "feature_counts = [result['n_features'] for result in results.values()]\n",
    "ax2.bar(design_names, feature_counts, color='steelblue', alpha=0.8)\n",
    "ax2.set_xlabel('Design', fontsize=12)\n",
    "ax2.set_ylabel('Number of Features', fontsize=12)\n",
    "ax2.set_title('Feature Dimensionality Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticklabels(design_names, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy comparison saved to 'accuracy_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdad42",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix cho Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d29fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Lấy best model từ section 5 (sẽ được update lại ở section 7.1)\n",
    "best_name = best_design[0]\n",
    "best_model = best_design[1]['model']\n",
    "\n",
    "# Tìm test data tương ứng\n",
    "for name, X_train, X_test in designs:\n",
    "    if name == best_name:\n",
    "        best_X_test = X_test\n",
    "        break\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_model.predict(best_X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(10), yticklabels=range(10),\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title(f'Confusion Matrix - {best_name}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved to 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=[str(i) for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d41c42",
   "metadata": {},
   "source": [
    "## 7.1. Detailed Metrics for All Designs\n",
    "\n",
    "Calculate comprehensive classification metrics for each feature design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Tính toán chi tiết metrics cho TẤT CẢ designs\n",
    "detailed_metrics = {}\n",
    "\n",
    "for name, X_train, X_test in designs:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Lấy model\n",
    "    model = results[name]['model']\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics cho từng class\n",
    "    # Training set metrics\n",
    "    train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n",
    "        y_train, y_train_pred, average=None, zero_division=0\n",
    "    )\n",
    "    train_precision_macro, train_recall_macro, train_f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_train, y_train_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    train_precision_weighted, train_recall_weighted, train_f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_train, y_train_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Test set metrics\n",
    "    test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_test_pred, average=None, zero_division=0\n",
    "    )\n",
    "    test_precision_macro, test_recall_macro, test_f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_test_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    test_precision_weighted, test_recall_weighted, test_f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_test, y_test_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrices\n",
    "    train_cm = confusion_matrix(y_train, y_train_pred)\n",
    "    test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    # Lưu vào dictionary\n",
    "    detailed_metrics[name] = {\n",
    "        'train': {\n",
    "            'accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'precision_per_class': train_precision,\n",
    "            'recall_per_class': train_recall,\n",
    "            'f1_per_class': train_f1,\n",
    "            'precision_macro': train_precision_macro,\n",
    "            'recall_macro': train_recall_macro,\n",
    "            'f1_macro': train_f1_macro,\n",
    "            'precision_weighted': train_precision_weighted,\n",
    "            'recall_weighted': train_recall_weighted,\n",
    "            'f1_weighted': train_f1_weighted,\n",
    "            'confusion_matrix': train_cm,\n",
    "            'predictions': y_train_pred\n",
    "        },\n",
    "        'test': {\n",
    "            'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "            'precision_per_class': test_precision,\n",
    "            'recall_per_class': test_recall,\n",
    "            'f1_per_class': test_f1,\n",
    "            'precision_macro': test_precision_macro,\n",
    "            'recall_macro': test_recall_macro,\n",
    "            'f1_macro': test_f1_macro,\n",
    "            'precision_weighted': test_precision_weighted,\n",
    "            'recall_weighted': test_recall_weighted,\n",
    "            'f1_weighted': test_f1_weighted,\n",
    "            'confusion_matrix': test_cm,\n",
    "            'predictions': y_test_pred\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTest Set Metrics:\")\n",
    "    print(f\"  Accuracy:           {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"  Precision (macro):  {test_precision_macro:.4f}\")\n",
    "    print(f\"  Recall (macro):     {test_recall_macro:.4f}\")\n",
    "    print(f\"  F1-Score (macro):   {test_f1_macro:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Detailed metrics calculation completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ff491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update best_design based on detailed_metrics for consistency\n",
    "best_design_updated = max(detailed_metrics.items(), key=lambda x: x[1]['test']['accuracy'])\n",
    "best_name = best_design_updated[0]\n",
    "best_metrics = best_design_updated[1]['test']\n",
    "best_model = results[best_name]['model']\n",
    "\n",
    "# Get corresponding test data\n",
    "for name, X_train, X_test in designs:\n",
    "    if name == best_name:\n",
    "        best_X_test = X_test\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL IDENTIFIED: {best_name}\")\n",
    "print(f\"Test Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793f632",
   "metadata": {},
   "source": [
    "## 7.2. Comprehensive Metrics Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a83450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo bảng so sánh chi tiết\n",
    "comparison_data = []\n",
    "\n",
    "for name in detailed_metrics.keys():\n",
    "    metrics = detailed_metrics[name]\n",
    "    comparison_data.append({\n",
    "        'Design': name.split(':')[0],  # Tên ngắn\n",
    "        'Features': results[name]['n_features'],\n",
    "        'Test Acc': metrics['test']['accuracy'],\n",
    "        'Test Prec (macro)': metrics['test']['precision_macro'],\n",
    "        'Test Rec (macro)': metrics['test']['recall_macro'],\n",
    "        'Test F1 (macro)': metrics['test']['f1_macro'],\n",
    "        'Test F1 (weighted)': metrics['test']['f1_weighted'],\n",
    "        'Train Acc': metrics['train']['accuracy'],\n",
    "        'Overfit Gap': metrics['train']['accuracy'] - metrics['test']['accuracy']\n",
    "    })\n",
    "\n",
    "# Tạo DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test Acc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"COMPREHENSIVE METRICS COMPARISON TABLE\")\n",
    "print(\"=\"*120)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Lưu ra CSV\n",
    "comparison_df.to_csv('metrics_comparison.csv', index=False)\n",
    "print(\"\\nTable saved to 'metrics_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e03db",
   "metadata": {},
   "source": [
    "## 7.3. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phân tích performance theo từng class (digit) cho từng design\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PER-CLASS PERFORMANCE ANALYSIS (Test Set)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for name in detailed_metrics.keys():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    metrics = detailed_metrics[name]['test']\n",
    "    \n",
    "    # Tạo DataFrame cho per-class metrics\n",
    "    per_class_df = pd.DataFrame({\n",
    "        'Digit': range(10),\n",
    "        'Precision': metrics['precision_per_class'],\n",
    "        'Recall': metrics['recall_per_class'],\n",
    "        'F1-Score': metrics['f1_per_class']\n",
    "    })\n",
    "    \n",
    "    print(per_class_df.to_string(index=False))\n",
    "    print(f\"\\nMacro Average: Precision={metrics['precision_macro']:.4f}, Recall={metrics['recall_macro']:.4f}, F1={metrics['f1_macro']:.4f}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12128b",
   "metadata": {},
   "source": [
    "## 7.4. Visualize Metrics Comparison Across Designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772dbdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive metrics visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "design_names = [name.split(':')[0] for name in detailed_metrics.keys()]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# 1. Overall Metrics Comparison (Accuracy, Precision, Recall, F1)\n",
    "ax1 = axes[0, 0]\n",
    "metrics_to_plot = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(design_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
    "    if metric == 'accuracy':\n",
    "        values = [detailed_metrics[name]['test']['accuracy'] for name in detailed_metrics.keys()]\n",
    "    else:\n",
    "        values = [detailed_metrics[name]['test'][metric] for name in detailed_metrics.keys()]\n",
    "    ax1.bar(x + i*width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Design', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Test Set: Overall Metrics Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x + width * 1.5)\n",
    "ax1.set_xticklabels(design_names, rotation=45, ha='right')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0.75, 1.0])  # Adjusted to show Design 4 (accuracy ~0.80)\n",
    "\n",
    "# 2. Train vs Test Accuracy Gap (Overfitting Analysis)\n",
    "ax2 = axes[0, 1]\n",
    "train_accs = [detailed_metrics[name]['train']['accuracy'] for name in detailed_metrics.keys()]\n",
    "test_accs = [detailed_metrics[name]['test']['accuracy'] for name in detailed_metrics.keys()]\n",
    "gaps = [train - test for train, test in zip(train_accs, test_accs)]\n",
    "\n",
    "x_pos = np.arange(len(design_names))\n",
    "bars = ax2.bar(x_pos, gaps, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Design', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy Gap (Train - Test)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Overfitting Analysis: Train-Test Gap', fontsize=13, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(design_names, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Weighted vs Macro F1-Score Comparison\n",
    "ax3 = axes[1, 0]\n",
    "f1_macro = [detailed_metrics[name]['test']['f1_macro'] for name in detailed_metrics.keys()]\n",
    "f1_weighted = [detailed_metrics[name]['test']['f1_weighted'] for name in detailed_metrics.keys()]\n",
    "\n",
    "x_pos = np.arange(len(design_names))\n",
    "width = 0.35\n",
    "ax3.bar(x_pos - width/2, f1_macro, width, label='Macro F1', alpha=0.8, color='steelblue')\n",
    "ax3.bar(x_pos + width/2, f1_weighted, width, label='Weighted F1', alpha=0.8, color='coral')\n",
    "ax3.set_xlabel('Design', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('F1-Score', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('F1-Score: Macro vs Weighted Average', fontsize=13, fontweight='bold')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(design_names, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Feature Count vs Test Accuracy\n",
    "ax4 = axes[1, 1]\n",
    "feature_counts = [results[name]['n_features'] for name in detailed_metrics.keys()]\n",
    "test_accs = [detailed_metrics[name]['test']['accuracy'] for name in detailed_metrics.keys()]\n",
    "\n",
    "scatter = ax4.scatter(feature_counts, test_accs, s=200, c=range(len(design_names)), \n",
    "                     cmap='viridis', alpha=0.7, edgecolors='black', linewidth=2)\n",
    "ax4.set_xlabel('Number of Features', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('Test Accuracy', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Feature Dimensionality vs Performance', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate points\n",
    "for i, (x, y, name) in enumerate(zip(feature_counts, test_accs, design_names)):\n",
    "    ax4.annotate(name, (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comprehensive_metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comprehensive metrics comparison saved to 'comprehensive_metrics_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95358d",
   "metadata": {},
   "source": [
    "## 7.5. Confusion Matrices for All Designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d314aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices cho TẤT CẢ designs\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, name in enumerate(detailed_metrics.keys()):\n",
    "    cm = detailed_metrics[name]['test']['confusion_matrix']\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(10), yticklabels=range(10),\n",
    "                cbar_kws={'label': 'Count'}, ax=axes[idx],\n",
    "                cbar=True, square=True)\n",
    "    \n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_title(f'{name.split(\":\")[0]}\\nAccuracy: {detailed_metrics[name][\"test\"][\"accuracy\"]:.4f}', \n",
    "                       fontsize=11, fontweight='bold')\n",
    "\n",
    "# Hide the last subplot if odd number of designs\n",
    "if len(detailed_metrics) < 6:\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"All confusion matrices saved to 'all_confusion_matrices.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f301e",
   "metadata": {},
   "source": [
    "## 7.6. Per-Class Performance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo heatmap so sánh F1-score theo từng class cho tất cả designs\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "# Prepare data\n",
    "design_names_short = [name.split(':')[0] for name in detailed_metrics.keys()]\n",
    "\n",
    "# F1-Score heatmap\n",
    "f1_data = np.array([detailed_metrics[name]['test']['f1_per_class'] \n",
    "                    for name in detailed_metrics.keys()])\n",
    "precision_data = np.array([detailed_metrics[name]['test']['precision_per_class'] \n",
    "                          for name in detailed_metrics.keys()])\n",
    "recall_data = np.array([detailed_metrics[name]['test']['recall_per_class'] \n",
    "                       for name in detailed_metrics.keys()])\n",
    "\n",
    "# Plot 1: F1-Score\n",
    "sns.heatmap(f1_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            xticklabels=range(10), yticklabels=design_names_short,\n",
    "            cbar_kws={'label': 'F1-Score'}, ax=axes[0], vmin=0.7, vmax=1.0)\n",
    "axes[0].set_xlabel('Digit Class', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Design', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('F1-Score per Digit Class', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Plot 2: Precision\n",
    "sns.heatmap(precision_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            xticklabels=range(10), yticklabels=design_names_short,\n",
    "            cbar_kws={'label': 'Precision'}, ax=axes[1], vmin=0.7, vmax=1.0)\n",
    "axes[1].set_xlabel('Digit Class', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Design', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Precision per Digit Class', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Plot 3: Recall\n",
    "sns.heatmap(recall_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            xticklabels=range(10), yticklabels=design_names_short,\n",
    "            cbar_kws={'label': 'Recall'}, ax=axes[2], vmin=0.7, vmax=1.0)\n",
    "axes[2].set_xlabel('Digit Class', fontsize=11, fontweight='bold')\n",
    "axes[2].set_ylabel('Design', fontsize=11, fontweight='bold')\n",
    "axes[2].set_title('Recall per Digit Class', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_performance_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Per-class performance heatmap saved to 'per_class_performance_heatmap.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33f988",
   "metadata": {},
   "source": [
    "## 7.7. Misclassification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phân tích lỗi phân loại cho best model (đã được định nghĩa ở section 7.1)\n",
    "best_cm = detailed_metrics[best_name]['test']['confusion_matrix']\n",
    "best_predictions = detailed_metrics[best_name]['test']['predictions']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MISCLASSIFICATION ANALYSIS - {best_name}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Tính tỷ lệ lỗi cho từng class\n",
    "print(\"Error Rate per Digit Class:\")\n",
    "print(\"-\" * 60)\n",
    "for digit in range(10):\n",
    "    total = best_cm[digit].sum()\n",
    "    correct = best_cm[digit, digit]\n",
    "    errors = total - correct\n",
    "    error_rate = (errors / total) * 100 if total > 0 else 0\n",
    "    print(f\"Digit {digit}: {errors:4d} errors / {total:4d} samples = {error_rate:5.2f}% error rate\")\n",
    "\n",
    "# Tìm các cặp digit bị nhầm lẫn nhiều nhất\n",
    "print(\"\\n\\nMost Common Misclassification Pairs:\")\n",
    "print(\"-\" * 60)\n",
    "misclass_pairs = []\n",
    "for true_label in range(10):\n",
    "    for pred_label in range(10):\n",
    "        if true_label != pred_label:\n",
    "            count = best_cm[true_label, pred_label]\n",
    "            if count > 0:\n",
    "                misclass_pairs.append((true_label, pred_label, count))\n",
    "\n",
    "# Sort by count\n",
    "misclass_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Show top 15\n",
    "print(f\"{'True Label':<12} {'Predicted As':<15} {'Count':<10} {'% of True Class'}\")\n",
    "print(\"-\" * 60)\n",
    "for true_label, pred_label, count in misclass_pairs[:15]:\n",
    "    total_true = best_cm[true_label].sum()\n",
    "    percentage = (count / total_true) * 100\n",
    "    print(f\"{true_label:<12} {pred_label:<15} {count:<10} {percentage:5.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771147d",
   "metadata": {},
   "source": [
    "## 7.8. Visualize Misclassified Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae92dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị ví dụ các ảnh bị phân loại sai\n",
    "misclassified_indices = np.where(y_test != best_predictions)[0]\n",
    "\n",
    "print(f\"Total misclassified samples: {len(misclassified_indices)} / {len(y_test)} ({len(misclassified_indices)/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# Lấy 20 ví dụ ngẫu nhiên\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(misclassified_indices, min(20, len(misclassified_indices)), replace=False)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = best_predictions[idx]\n",
    "    \n",
    "    # Get original image\n",
    "    img = x_test_flat[idx].reshape(28, 28)\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'True: {true_label} → Pred: {pred_label}',\n",
    "                      color='red', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Misclassified Examples - {best_name}', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('misclassified_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Misclassified examples saved to 'misclassified_examples.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572af8dc",
   "metadata": {},
   "source": [
    "## 7.9. Model Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273156ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phân tích confidence của model (prediction probability)\n",
    "# Sử dụng best_model và best_X_test đã được định nghĩa ở section 7.1\n",
    "y_pred_proba = best_model.predict_proba(best_X_test)\n",
    "y_pred_confidence = y_pred_proba.max(axis=1)\n",
    "\n",
    "# Phân loại predictions theo confidence level\n",
    "correct_mask = (y_test == best_predictions)\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "correct_confidence = y_pred_confidence[correct_mask]\n",
    "incorrect_confidence = y_pred_confidence[incorrect_mask]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PREDICTION CONFIDENCE ANALYSIS - {best_name}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Correct Predictions ({len(correct_confidence)} samples):\")\n",
    "print(f\"  Mean confidence: {correct_confidence.mean():.4f}\")\n",
    "print(f\"  Median confidence: {np.median(correct_confidence):.4f}\")\n",
    "print(f\"  Std confidence: {correct_confidence.std():.4f}\")\n",
    "print(f\"  Min confidence: {correct_confidence.min():.4f}\")\n",
    "print(f\"  Max confidence: {correct_confidence.max():.4f}\")\n",
    "\n",
    "print(f\"\\nIncorrect Predictions ({len(incorrect_confidence)} samples):\")\n",
    "print(f\"  Mean confidence: {incorrect_confidence.mean():.4f}\")\n",
    "print(f\"  Median confidence: {np.median(incorrect_confidence):.4f}\")\n",
    "print(f\"  Std confidence: {incorrect_confidence.std():.4f}\")\n",
    "print(f\"  Min confidence: {incorrect_confidence.min():.4f}\")\n",
    "print(f\"  Max confidence: {incorrect_confidence.max():.4f}\")\n",
    "\n",
    "# Visualize confidence distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(correct_confidence, bins=50, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
    "axes[0].hist(incorrect_confidence, bins=50, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
    "axes[0].set_xlabel('Prediction Confidence', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Confidence Distribution: Correct vs Incorrect Predictions', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [correct_confidence, incorrect_confidence]\n",
    "bp = axes[1].boxplot(data_to_plot, labels=['Correct', 'Incorrect'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('green')\n",
    "bp['boxes'][0].set_alpha(0.5)\n",
    "bp['boxes'][1].set_facecolor('red')\n",
    "bp['boxes'][1].set_alpha(0.5)\n",
    "axes[1].set_ylabel('Prediction Confidence', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Confidence Box Plot Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfidence analysis saved to 'confidence_analysis.png'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e254a8",
   "metadata": {},
   "source": [
    "## 7.10. Detailed Interpretation and Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive interpretation report\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION AND ANALYSIS REPORT\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n\" + \"─\"*100)\n",
    "print(\"1. OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"─\"*100)\n",
    "\n",
    "# Sort designs by test accuracy\n",
    "sorted_designs = sorted(detailed_metrics.items(), key=lambda x: x[1]['test']['accuracy'], reverse=True)\n",
    "\n",
    "for rank, (name, metrics) in enumerate(sorted_designs, 1):\n",
    "    print(f\"\\nRank {rank}: {name}\")\n",
    "    print(f\"  Test Accuracy:     {metrics['test']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro): {metrics['test']['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (macro):    {metrics['test']['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (macro):  {metrics['test']['f1_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (wtd):    {metrics['test']['f1_weighted']:.4f}\")\n",
    "    print(f\"  Feature Count:     {results[name]['n_features']}\")\n",
    "    print(f\"  Overfitting Gap:   {metrics['train']['accuracy'] - metrics['test']['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"─\"*100)\n",
    "print(\"2. KEY OBSERVATIONS FROM METRICS\")\n",
    "print(\"─\"*100)\n",
    "\n",
    "# Calculate statistics\n",
    "all_test_accs = [m['test']['accuracy'] for m in detailed_metrics.values()]\n",
    "all_gaps = [m['train']['accuracy'] - m['test']['accuracy'] for m in detailed_metrics.values()]\n",
    "all_features = [results[n]['n_features'] for n in detailed_metrics.keys()]\n",
    "\n",
    "print(f\"\\nPerformance Range:\")\n",
    "print(f\"  Best Test Accuracy:  {max(all_test_accs):.4f} ({sorted_designs[0][0].split(':')[0]})\")\n",
    "print(f\"  Worst Test Accuracy: {min(all_test_accs):.4f}\")\n",
    "print(f\"  Performance Spread:  {max(all_test_accs) - min(all_test_accs):.4f}\")\n",
    "\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "print(f\"  Smallest Gap:  {min(all_gaps):.4f}\")\n",
    "print(f\"  Largest Gap:   {max(all_gaps):.4f}\")\n",
    "print(f\"  Average Gap:   {np.mean(all_gaps):.4f}\")\n",
    "\n",
    "print(f\"\\nFeature Dimensionality:\")\n",
    "print(f\"  Minimum Features:  {min(all_features)}\")\n",
    "print(f\"  Maximum Features:  {max(all_features)}\")\n",
    "\n",
    "print(\"\\n\" + \"─\"*100)\n",
    "print(\"3. PER-CLASS PERFORMANCE INSIGHTS (Best Model)\")\n",
    "print(\"─\"*100)\n",
    "\n",
    "# Use the already defined best_metrics from section 7.1\n",
    "print(f\"\\nBest Model: {best_name}\")\n",
    "print(f\"\\nDigit-wise Performance:\")\n",
    "for digit in range(10):\n",
    "    f1 = best_metrics['f1_per_class'][digit]\n",
    "    precision = best_metrics['precision_per_class'][digit]\n",
    "    recall = best_metrics['recall_per_class'][digit]\n",
    "    \n",
    "    # Count samples for this digit\n",
    "    digit_samples = (y_test == digit).sum()\n",
    "    digit_correct = best_metrics['confusion_matrix'][digit, digit]\n",
    "    digit_accuracy = digit_correct / digit_samples if digit_samples > 0 else 0\n",
    "    \n",
    "    print(f\"  Digit {digit}: F1={f1:.4f}, Prec={precision:.4f}, Rec={recall:.4f}, Acc={digit_accuracy:.4f} ({digit_samples} samples)\")\n",
    "\n",
    "# Find best and worst performing digits\n",
    "f1_scores = best_metrics['f1_per_class']\n",
    "best_digit = np.argmax(f1_scores)\n",
    "worst_digit = np.argmin(f1_scores)\n",
    "\n",
    "print(f\"\\n  Best Performing Digit:  {best_digit} (F1={f1_scores[best_digit]:.4f})\")\n",
    "print(f\"  Worst Performing Digit: {worst_digit} (F1={f1_scores[worst_digit]:.4f})\")\n",
    "print(f\"  Performance Gap:        {f1_scores[best_digit] - f1_scores[worst_digit]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"─\"*100)\n",
    "print(\"4. FEATURE DESIGN IMPACT ANALYSIS\")\n",
    "print(\"─\"*100)\n",
    "\n",
    "print(\"\\nObserved Relationships:\")\n",
    "# Correlation between features and performance\n",
    "feature_counts = [results[n]['n_features'] for n in detailed_metrics.keys()]\n",
    "test_accuracies = [m['test']['accuracy'] for m in detailed_metrics.values()]\n",
    "correlation = np.corrcoef(feature_counts, test_accuracies)[0, 1]\n",
    "\n",
    "print(f\"  Feature Count vs Accuracy Correlation: {correlation:.4f}\")\n",
    "if abs(correlation) < 0.3:\n",
    "    print(f\"    → Weak correlation: More features ≠ better performance\")\n",
    "elif correlation > 0:\n",
    "    print(f\"    → Positive correlation: More features tend to improve performance\")\n",
    "else:\n",
    "    print(f\"    → Negative correlation: Fewer features may perform better\")\n",
    "\n",
    "# Analyze each design's characteristics\n",
    "print(\"\\nDesign-Specific Analysis:\")\n",
    "for name in detailed_metrics.keys():\n",
    "    design_short = name.split(':')[0]\n",
    "    acc = detailed_metrics[name]['test']['accuracy']\n",
    "    gap = detailed_metrics[name]['train']['accuracy'] - detailed_metrics[name]['test']['accuracy']\n",
    "    n_feat = results[name]['n_features']\n",
    "    \n",
    "    print(f\"\\n  {design_short}:\")\n",
    "    print(f\"    Features: {n_feat}\")\n",
    "    print(f\"    Test Acc: {acc:.4f}\")\n",
    "    print(f\"    Overfitting: {'Low' if gap < 0.01 else 'Moderate' if gap < 0.03 else 'High'} (gap={gap:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"─\"*100)\n",
    "print(\"5. STRENGTHS AND WEAKNESSES OF SOFTMAX REGRESSION\")\n",
    "print(\"─\"*100)\n",
    "\n",
    "print(\"\\nStrengths (observed from results):\")\n",
    "print(f\"  • High overall accuracy achieved: {max(all_test_accs):.4f} on test set\")\n",
    "print(f\"  • Consistent performance across most digit classes\")\n",
    "print(f\"  • Relatively low overfitting (average gap: {np.mean(all_gaps):.4f})\")\n",
    "print(f\"  • Fast convergence within {EPOCHS} epochs\")\n",
    "print(f\"  • Stable predictions with high confidence for correct classifications\")\n",
    "\n",
    "print(\"\\nWeaknesses (observed from results):\")\n",
    "# Identify common misclassifications\n",
    "print(f\"  • Performance varies across digit classes (F1 range: {f1_scores.min():.4f} - {f1_scores.max():.4f})\")\n",
    "print(f\"  • Some digit pairs are frequently confused (see confusion matrix)\")\n",
    "print(f\"  • Misclassified samples show lower confidence but still significant\")\n",
    "print(f\"  • Linear decision boundaries may not capture complex patterns\")\n",
    "\n",
    "# Calculate macro vs weighted F1 difference\n",
    "macro_f1 = best_metrics['f1_macro']\n",
    "weighted_f1 = best_metrics['f1_weighted']\n",
    "print(f\"  • Class imbalance sensitivity: Macro F1={macro_f1:.4f} vs Weighted F1={weighted_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"─\"*100)\n",
    "print(\"6. CONFUSION PATTERNS ANALYSIS\")\n",
    "print(\"─\"*100)\n",
    "\n",
    "# Analyze confusion matrix patterns\n",
    "cm = best_metrics['confusion_matrix']\n",
    "print(f\"\\nMost Confused Digit Pairs (Best Model):\")\n",
    "\n",
    "confusion_pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append((i, j, cm[i, j], cm[i, j] / cm[i].sum() * 100))\n",
    "\n",
    "confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for true_digit, pred_digit, count, percentage in confusion_pairs[:10]:\n",
    "    print(f\"  {true_digit} → {pred_digit}: {count:3d} times ({percentage:4.1f}% of all {true_digit}'s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee8e6d",
   "metadata": {},
   "source": [
    "## 7.11. Export Summary Statistics for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1044763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export detailed statistics to files for easy reporting\n",
    "Path('evaluation_results').mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Per-class metrics for all designs\n",
    "for name in detailed_metrics.keys():\n",
    "    design_short = name.split(':')[0].replace(' ', '_')\n",
    "    per_class_data = []\n",
    "    \n",
    "    metrics = detailed_metrics[name]['test']\n",
    "    for digit in range(10):\n",
    "        per_class_data.append({\n",
    "            'Digit': digit,\n",
    "            'Precision': metrics['precision_per_class'][digit],\n",
    "            'Recall': metrics['recall_per_class'][digit],\n",
    "            'F1-Score': metrics['f1_per_class'][digit],\n",
    "            'Support': (y_test == digit).sum()\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(per_class_data)\n",
    "    df.to_csv(f'evaluation_results/{design_short}_per_class_metrics.csv', index=False)\n",
    "\n",
    "# 2. Overall comparison summary\n",
    "summary_data = []\n",
    "for name in detailed_metrics.keys():\n",
    "    design_short = name.split(':')[0]\n",
    "    train_m = detailed_metrics[name]['train']\n",
    "    test_m = detailed_metrics[name]['test']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Design': design_short,\n",
    "        'Features': results[name]['n_features'],\n",
    "        'Train_Accuracy': train_m['accuracy'],\n",
    "        'Test_Accuracy': test_m['accuracy'],\n",
    "        'Test_Precision_Macro': test_m['precision_macro'],\n",
    "        'Test_Recall_Macro': test_m['recall_macro'],\n",
    "        'Test_F1_Macro': test_m['f1_macro'],\n",
    "        'Test_Precision_Weighted': test_m['precision_weighted'],\n",
    "        'Test_Recall_Weighted': test_m['recall_weighted'],\n",
    "        'Test_F1_Weighted': test_m['f1_weighted'],\n",
    "        'Overfitting_Gap': train_m['accuracy'] - test_m['accuracy']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Test_Accuracy', ascending=False)\n",
    "summary_df.to_csv('evaluation_results/overall_comparison.csv', index=False)\n",
    "\n",
    "# 3. Confusion matrix for best model\n",
    "best_cm_df = pd.DataFrame(\n",
    "    best_metrics['confusion_matrix'],\n",
    "    index=[f'True_{i}' for i in range(10)],\n",
    "    columns=[f'Pred_{i}' for i in range(10)]\n",
    ")\n",
    "best_cm_df.to_csv(f'evaluation_results/best_model_confusion_matrix.csv')\n",
    "\n",
    "# 4. Misclassification pairs\n",
    "misclass_data = []\n",
    "for true_digit, pred_digit, count, percentage in confusion_pairs[:20]:\n",
    "    misclass_data.append({\n",
    "        'True_Label': true_digit,\n",
    "        'Predicted_Label': pred_digit,\n",
    "        'Count': count,\n",
    "        'Percentage_of_True_Class': percentage\n",
    "    })\n",
    "misclass_df = pd.DataFrame(misclass_data)\n",
    "misclass_df.to_csv('evaluation_results/misclassification_pairs.csv', index=False)\n",
    "\n",
    "# 5. Confidence statistics\n",
    "confidence_stats = {\n",
    "    'Metric': ['Mean', 'Median', 'Std', 'Min', 'Max'],\n",
    "    'Correct_Predictions': [\n",
    "        correct_confidence.mean(),\n",
    "        np.median(correct_confidence),\n",
    "        correct_confidence.std(),\n",
    "        correct_confidence.min(),\n",
    "        correct_confidence.max()\n",
    "    ],\n",
    "    'Incorrect_Predictions': [\n",
    "        incorrect_confidence.mean(),\n",
    "        np.median(incorrect_confidence),\n",
    "        incorrect_confidence.std(),\n",
    "        incorrect_confidence.min(),\n",
    "        incorrect_confidence.max()\n",
    "    ]\n",
    "}\n",
    "confidence_df = pd.DataFrame(confidence_stats)\n",
    "confidence_df.to_csv('evaluation_results/confidence_statistics.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS EXPORTED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFiles saved in 'evaluation_results/' directory:\")\n",
    "print(\"  • overall_comparison.csv - Complete metrics comparison\")\n",
    "print(\"  • [design]_per_class_metrics.csv - Per-class metrics for each design\")\n",
    "print(\"  • best_model_confusion_matrix.csv - Confusion matrix\")\n",
    "print(\"  • misclassification_pairs.csv - Most common errors\")\n",
    "print(\"  • confidence_statistics.csv - Prediction confidence analysis\")\n",
    "print(\"\\nAll visualization files saved in root directory:\")\n",
    "print(\"  • comprehensive_metrics_comparison.png\")\n",
    "print(\"  • all_confusion_matrices.png\")\n",
    "print(\"  • per_class_performance_heatmap.png\")\n",
    "print(\"  • misclassified_examples.png\")\n",
    "print(\"  • confidence_analysis.png\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c094d37",
   "metadata": {},
   "source": [
    "## 8. Lưu Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc10f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục saved_models nếu chưa có\n",
    "Path('saved_models').mkdir(exist_ok=True)\n",
    "\n",
    "# Lưu tất cả models\n",
    "for name, result in results.items():\n",
    "    # Tạo filename từ design name\n",
    "    filename = name.lower().replace(' ', '_').replace(':', '').replace('(', '').replace(')', '') + '.pkl'\n",
    "    filepath = Path('saved_models') / filename\n",
    "    \n",
    "    # Lưu model\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(result['model'], f)\n",
    "    \n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "# Lưu best model riêng\n",
    "with open('saved_models/best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"\\nBest model saved: saved_models/best_model.pkl\")\n",
    "\n",
    "# Lưu metadata\n",
    "metadata = {\n",
    "    'best_design': best_name,\n",
    "    'test_accuracy': best_metrics['accuracy'],\n",
    "    'train_accuracy': detailed_metrics[best_name]['train']['accuracy'],\n",
    "    'hyperparameters': {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'reg': REG,\n",
    "        'random_state': RANDOM_STATE\n",
    "    },\n",
    "    'all_results': {\n",
    "        name: {\n",
    "            'train_accuracy': result['train_accuracy'],\n",
    "            'test_accuracy': result['test_accuracy'],\n",
    "            'n_features': result['n_features']\n",
    "        }\n",
    "        for name, result in results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('saved_models/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"Metadata saved: saved_models/metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf93750",
   "metadata": {},
   "source": [
    "## 9. Test Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy 10 mẫu ngẫu nhiên từ test set\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(y_test), 10, replace=False)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get sample\n",
    "    sample = best_X_test[idx:idx+1]\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    # Predict\n",
    "    pred_label = best_model.predict(sample)[0]\n",
    "    pred_probs = best_model.predict_proba(sample)[0]\n",
    "    confidence = pred_probs.max()\n",
    "    \n",
    "    # Get original image\n",
    "    img = x_test_flat[idx].reshape(28, 28)\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Title with color based on correctness\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    axes[i].set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2f}',\n",
    "                      color=color, fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction examples saved to 'prediction_examples.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f15f21",
   "metadata": {},
   "source": [
    "## 10. Summary và Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nBest Performing Design: {best_name}\")\n",
    "print(f\"   - Test Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"   - Train Accuracy: {detailed_metrics[best_name]['train']['accuracy']:.4f}\")\n",
    "print(f\"   - Number of Features: {results[best_name]['n_features']}\")\n",
    "print(f\"   - Overfitting Gap: {detailed_metrics[best_name]['train']['accuracy'] - best_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nAll Results (sorted by test accuracy):\")\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['test_accuracy'], reverse=True)\n",
    "for i, (name, result) in enumerate(sorted_results, 1):\n",
    "    print(f\"   {i}. {name}\")\n",
    "    print(f\"      Test Acc: {result['test_accuracy']:.4f} | Features: {result['n_features']}\")\n",
    "\n",
    "print(f\"\\nSaved Files:\")\n",
    "print(f\"   - Models: saved_models/\")\n",
    "print(f\"   - Best Model: saved_models/best_model.pkl\")\n",
    "print(f\"   - Metadata: saved_models/metadata.pkl\")\n",
    "print(f\"   - Visualizations: training_curves.png, accuracy_comparison.png, etc.\")\n",
    "\n",
    "print(\"\\nTraining Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
